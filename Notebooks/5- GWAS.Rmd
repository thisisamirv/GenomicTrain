---
title: "GWAS"
output: 
  pdf_document: 
    number_sections: yes
    toc: yes
header-includes: 
  - \usepackage{graphics}
  - \usepackage{float}
editor_options: 
  markdown: 
    wrap: sentence
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE, message = FALSE)
rm(list = ls())
```

\newpage

# GWAS based on SNPs
## Scenarios

1- We will work with a small sample from a real dataset from sheep. The platform is an Illumina SNP chip with 20,000 SNP and the data file was generated by Genome-Studio (proprietary software from Illumina). We have 83 animals genotyped.

2- It includes 100,000 SNPs that have been simulated using real data from a case-control study. Our phenotype of interest is obesity (0: not obese; 1: obese) that has been created using body mass index information of each individual.

3- The sample data includes RNA from 12 randomly selected mice from two strains, and two pools with the RNA from all twelve mice from each of the two strains. Each row represents a sample and the columns are the mice. The first row represents the strain. A "1" in cell i,j indicates that RNA from mouse j was included in sample i.

## Load data
Let's load the data for both scenarios:
```{r}
# Scenario 1 (S1) - Import data
annotation_s1 <-
  read.table("Data/SNPmap.txt", header = TRUE, sep = "\t")
g1 <- read.table("Data/SNPSample1.txt", header = TRUE, sep = "\t")
g2 <- read.table("Data/SNPSample2.txt", header = TRUE, sep = "\t")
g3 <- read.table("Data/SNPSample3.txt", header = TRUE, sep = "\t")
g4 <- read.table("Data/SNPSample4.txt", header = TRUE, sep = "\t")
g5 <- read.table("Data/SNPSample5.txt", header = TRUE, sep = "\t")
geno_s1 <- rbind(g1, g2, g3, g4, g5)
rm(g1, g2, g3, g4, g5)

# Create genotype column
geno_s1 = data.frame(geno_s1, genotype = factor(
  paste(geno_s1$allele1, geno_s1$allele2, sep = ""),
  levels = c("AA", "AB", "BB")
))

# Number of samples
d1_s1 = dim(geno_s1)[1]

# Retrieve data for one SNP
first_snp_s1 <- geno_s1[geno_s1$snp == annotation_s1$name[1],]

# Check its data for the first 3 samples
first_snp_s1[1:3,]
```

```{bash, echo=FALSE}
tar -zxvf Data/obesity.bed.tar.gz -C Output/
```

```{r}
# Scenario 2 (S2) - Import data
# Import packages
library(snpStats)

# Import genotype, annotation, and family data
ob.plink_s2 <-
  read.plink(bed = "Output/obesity.bed",
             bim = "Data/obesity.bim",
             fam = "Data/obesity.fam")

geno_s2 <- ob.plink_s2$genotypes
annotation_s2 <- ob.plink_s2$map
family_s2 <- ob.plink_s2$fam

# Import phenotype data
pheno_s2 <- read.delim("Data/obesity.txt")

# Rename the rows of “pheno” with the “id” variable
rownames(pheno_s2) <- pheno_s2$id

# Select the common individuals between the files
ids_s2 <- intersect(rownames(pheno_s2), rownames(geno_s2))
geno_s2 <- geno_s2[ids_s2,]
pheno_s2 <- pheno_s2[ids_s2,]
family_s2 <- family_s2[ids_s2,]

# Retrieve data for one SNP
first_snp_s2 <- col.summary(geno_s2[1])["rs2722784",]
for (i in 2:dim(geno_s2)[1]) {
  first_snp_s2 <-
    rbind(first_snp_s2, col.summary(geno_s2[i])["rs2722784",])
}

# Check its data for the first 3 samples
first_snp_s2[1:3,]
```

```{r}
# Scenario 3 (S3) - Import data
library(Biobase)
library(maPooling)
data(maPooling)
pd = pData(maPooling)
individuals = which(rowSums(pd) == 1)
individuals = individuals[-grep("tr", names(individuals))]
ind_y = exprs(maPooling)[, individuals]
ind_g = factor(as.numeric(grepl("b", names(individuals))))
ind_y[1:3, 1:3]
ind_g
```


## Pre-processing and Quality Control
it involves:

* QC of SNPs
* QC of individuals
* QC of Population ancestry

### QC of SNPs

* Check normalized intensities: Our 1st scenario data also has an X and a Y column (these are specific to Illumina). These are the normalized intensities of the reads for each of the two alleles. We would expect that one of the homozygous genotypes would show high X values and low Y values while the other homozygote would be the opposite. Heterozygotes would be somewhere between the two.
* Remove SNPs with a low call rate
* Remove rare SNPs (e.g. having low minor allele frequency - MAF): Note that not all SNP will be polymorphic, some will show only one allele across all samples (monomorphic) or one of the alleles will be at a very low frequency. The association between a phenotype and a rare allele might be supported by only very few individuals (no power to detect the association), in this case the results should be interpreted with caution. To avoid this potential problem, SNP filtering based on MAF is often used to exclude low MAF SNP (usual thresholds are between 1% and 5%).
* Remove SNPs that do not pass the HWE test: The HWE law describes the relationship between genotypic frequencies and allelic frequencies and how they remain constant across generations in a population of diploid sexually reproducing organisms under the assumptions of random mating and an infinitely large population. When empirical observations are in a statistical sense significantly different from the model’s predictions, there is a strong indication that some biologically relevant factor is acting on this population or there are genotyping errors in the data. Common p-value thresholds for HWE is 0.0001 or less.
* Remove SNPs with low GC score: It is a measure of how reliable the call is on a scale from 0 to 1. Some labs will not assign a call to GC scores under 0.25. Another common magic number is to remove reads under 0.6 (and projects working with human data may use even higher thresholds of 0.7–0.8).

#### Check normalized intensities
At one SNP level:
```{r}
# S1 - Check normalized intensities
snp_s1 = data.frame(first_snp_s1, genotype = factor(
  paste(first_snp_s1$allele1, first_snp_s1$allele2, sep = ""),
  levels = c("AA", "AB", "BB")
))

plot(
  snp_s1$x,
  snp_s1$y,
  col = snp_s1$genotype,
  pch = as.numeric(snp_s1$genotype),
  xlab = "x",
  ylab = "y",
  main = "First SNP",
  cex.main = 0.9
)

legend(
  "bottomleft",
  paste(
    levels(snp_s1$genotype),
    " (",
    summary(snp_s1$genotype),
    ")",
    sep = ""
  ),
  col = 1:length(levels(snp_s1$genotype)),
  pch = 1:length(levels(snp_s1$genotype)),
  cex = 0.7
)
```

As discussed before, notice how the genotypes clearly cluster into three discrete groups—an indication of good data. Common practice is to go back to these plots after the association test and make sure that the SNP data looks ok at least for the significant SNP.

#### Remove SNPs with a low call rate
At genome-wide level:
```{r}
# S2 - Remove SNPs with a low call rate
# Import packages
library(snpStats)
# Remove SNPs with a call rate < 95%
info_snps_s2 <- col.summary(geno_s2)
use_s2 <- info_snps_s2$Call.rate > 0.95
mask.snps_s2 <- use_s2 & !is.na(use_s2)
geno_s2 <- geno_s2[, mask.snps_s2]
annotation_s2 <- annotation_s2[mask.snps_s2, ]

# Number of SNPs removed for bad call rate
sum(info_snps_s2$Call.rate < 0.95)
```

#### Remove rare SNPs (low MAF)
SNP alleles are usually coded as A/B in Illumina.

At one SNP level:
```{r}
# S1 - Check one SNP allele frequency
first_snp_s1$allele1 <- factor(first_snp_s1$allele1)
first_snp_s1$allele2 <- factor(first_snp_s1$allele2)
alleles_s1 <- c(first_snp_s1$allele1, first_snp_s1$allele2)
summary(alleles_s1) / sum(summary(alleles_s1))
```

```{r}
# S2 - Check one SNP allele frequency
# Reterive allels frequency
for (i in 1:dim(geno_s2)[1]) {
  if (isTRUE(first_snp_s2[i, 6] == 1)) {
    first_snp_s2[i, 10] = "A"
    first_snp_s2[i, 11] = "A"
  }
  else if (isTRUE(first_snp_s2[i, 7] == 1)) {
    first_snp_s2[i, 10] = "A"
    first_snp_s2[i, 11] = "B"
  }
  else if (isTRUE(first_snp_s2[i, 8] == 1)) {
    first_snp_s2[i, 10] = "B"
    first_snp_s2[i, 11] = "B"
  }
  else {
    first_snp_s2[i, 10] = NA
    first_snp_s2[i, 11] = NA
  }
}

first_snp_s2[, 10] <- factor(first_snp_s2[, 10])
first_snp_s2[, 11] <- factor(first_snp_s2[, 11])

# Rename columns
colnames(first_snp_s2)[10:11] <- c("first_allele", "second_allele")

# Remove rows with missing genotype
first_snp_s2 <- first_snp_s2[!is.na(first_snp_s2$first_allele), ]
first_snp_s2 <- first_snp_s2[!is.na(first_snp_s2$second_allele), ]

# Check allels frequency
alleles_s2 = c(first_snp_s2[, 10], first_snp_s2[, 11])
summary(alleles_s2) / sum(summary(alleles_s2))
```

For S1: The frequencies are reasonable, around one-quarter A allele and three-quarters B allele.
For S2: The frequencies are reasonable, around one-third A allele and two-thirds B allele.

At genome-wide level:
```{r}
# S1 - Remove SNPs with MAF < 0.05 for the first 20 SNPs
for (i in 1:20) {
  one_snp_s1 = geno_s1[geno_s1$snp == annotation_s1$name[i],]
  one_snp_s1$allele1 = factor(one_snp_s1$allele1)
  one_snp_s1$allele2 = factor(one_snp_s1$allele2)
  one_snp_alleles_s1 = c(one_snp_s1$allele1, one_snp_s1$allele2)
  AF_s1 <-
    summary(one_snp_alleles_s1) / sum(summary(one_snp_alleles_s1))
  if (isTRUE(AF_s1[1] < 0.05)) {
    geno_s1 <- geno_s1[geno_s1$snp != annotation_s1$name[i], ]
    annotation_s1 <-
      annotation_s1[annotation_s1$name != annotation_s1$name[i], ]
  }
  else if (isTRUE(AF_s1[2] < 0.05)) {
    geno_s1 <- geno_s1[geno_s1$snp != annotation_s1$name[i], ]
    annotation_s1 <-
      annotation_s1[annotation_s1$name != annotation_s1$name[i], ]
  }
}

# SNPs removed
d2_s1 = dim(geno_s1)[1]
print((d1_s1 - d2_s1) / 83)
```

```{r}
# S2 - Remove SNPs with MAF < 0.05
info_snps_s2 <- col.summary(geno_s2)
use_s2 <- info_snps_s2$MAF > 0.05
mask.snps_s2 <- use_s2 & !is.na(use_s2)
geno_s2 <- geno_s2[, mask.snps_s2]
annotation_s2 <- annotation_s2[mask.snps_s2, ]

# Number of SNPs removed for low MAF
sum(info_snps_s2$MAF < 0.05, na.rm = TRUE)
```

#### Remove SNPs that do not pass the HWE test
At one SNP level:

```{r}
# S1 - Calculate HWE for the 1st SNP
first_snp_s1$genotype <- factor(first_snp_s1$genotype)

# Remove rows with missing genotype
first_snp_s1 <- first_snp_s1[!is.na(first_snp_s1$genotype), ]

# Remove "-" level
first_snp_s1$genotype <-
  droplevels(first_snp_s1$genotype, exclude = "-")

# Calculate HWE
obs_s1 = summary(first_snp_s1$genotype)
hwal_s1 = summary(alleles_s1) / sum(summary(alleles_s1))

# Calculate its p-value (chi-square test with yates correction)
exp_s1 = c(hwal_s1[1] ^ 2, 2 * hwal_s1[1] * hwal_s1[2],
           hwal_s1[2] ^ 2) * sum(obs_s1)
names(exp_s1) = c("AA", "AB", "BB")

# Plot expected vs observed
par(mfrow = c(1, 2))
pl1_s1 <-
  barplot(exp_s1,
          col = "blue",
          main = "Expected",
          ylim = c(0, 50))
text(
  x = pl1_s1,
  y = exp_s1,
  label = round(exp_s1, 3),
  pos = 3,
  cex = 0.8,
  col = "red"
)
pl2_s1 <- barplot(obs_s1,
                  col = "red",
                  main = "Observed",
                  ylim = c(0, 50))
text(
  x = pl2_s1,
  y = obs_s1,
  label = round(obs_s1, 3),
  pos = 3,
  cex = 0.8,
  col = "blue"
)

# Perform chi-square test
xtot_s1 = sum((abs(obs_s1 - exp_s1) - c(0.5, 1, 0.5)) ^ 2 / exp_s1)
pval_s1 = 1 - pchisq(xtot_s1, 1)
print(pval_s1)
```

```{r}
# S2 - Calculate HWE for the 1st SNP
# Reterive genotype
for (i in 1:dim(geno_s2)[1]) {
  if (isTRUE(first_snp_s2[i, 6] == 1)) {
    first_snp_s2[i, 12] = "AA"
  }
  else if (isTRUE(first_snp_s2[i, 7] == 1)) {
    first_snp_s2[i, 12] = "AB"
  }
  else if (isTRUE(first_snp_s2[i, 8] == 1)) {
    first_snp_s2[i, 12] = "BB"
  }
  else {
    first_snp_s2[i, 12] = NA
  }
}

colnames(first_snp_s2)[12] <- "genotype"
first_snp_s2[, 12] <- factor(first_snp_s2[, 12])

# Remove rows with missing genotype
first_snp_s2 <- first_snp_s2[!is.na(first_snp_s2$genotype), ]

# Remove "-" level
first_snp_s2[, 12] <- droplevels(first_snp_s2[, 12], exclude = "-")

# Calculate HWE
obs_s2 = summary(first_snp_s2$genotype)
hwal_s2 = summary(alleles_s2) / sum(summary(alleles_s2))

# Calculate its p-value (chi-square test with yates correction)
exp_s2 = c(hwal_s2[1] ^ 2, 2 * hwal_s2[1] * hwal_s2[2],
           hwal_s2[2] ^ 2) * sum(obs_s2)
names(exp_s2) = c("AA", "AB", "BB")

# Plot expected vs observed
par(mfrow = c(1, 2))
pl1_s2 <-
  barplot(exp_s2,
          col = "blue",
          main = "Expected",
          ylim = c(0, 550))
text(
  x = pl1_s2,
  y = exp_s2,
  label = round(exp_s2, 3),
  pos = 3,
  cex = 0.8,
  col = "red"
)
pl2_s2 <- barplot(obs_s2,
                  col = "red",
                  main = "Observed",
                  ylim = c(0, 550))
text(
  x = pl2_s2,
  y = obs_s2,
  label = round(obs_s2, 3),
  pos = 3,
  cex = 0.8,
  col = "blue"
)

# Perform chi-square test
xtot_s2 = sum((abs(obs_s2 - exp_s2) - c(0.5, 1, 0.5)) ^ 2 / exp_s2)
pval_s2 = 1 - pchisq(xtot_s2, 1)
print(pval_s2)
```

The SNPs in both scenarios are in Hardy–Weinberg equilibrium.

At genome-wide level:
```{r}
# S1 - Remove SNPs with p < 0.0001 for the first 20 SNPs
for (i in 1:20) {
  one_snp_s1 = geno_s1[geno_s1$snp == annotation_s1$name[i], ]
  ## Calculate HWE for the SNP
  obs_s1 = summary(one_snp_s1$genotype)
  one_snp_s1$allele1 = factor(one_snp_s1$allele1)
  one_snp_s1$allele2 = factor(one_snp_s1$allele2)
  one_snp_alleles_s1 = c(one_snp_s1$allele1, one_snp_s1$allele2)
  hwal_s1 <-
    summary(one_snp_alleles_s1) / sum(summary(one_snp_alleles_s1))
  ## Calculate its p-value (chi-square test with yates correction)
  exp_s1 = c(hwal_s1[1] ^ 2, 2 * hwal_s1[1] * hwal_s1[2], hwal_s1[2] ^ 2) * sum(obs_s1)
  names(exp_s1) = c("AA", "AB", "BB")
  ## Perform chi-square test
  xtot_s1 = sum((abs(obs_s1 - exp_s1) - c(0.5, 1, 0.5)) ^ 2 / exp_s1)
  pval_s1 = 1 - pchisq(xtot_s1, 1)
  if (isTRUE(pval_s1 < 0.0001)) {
    geno_s1 <- geno_s1[geno_s1$snp != annotation_s1$name[i], ]
    annotation_s1 <-
      annotation_s1[annotation_s1$name != annotation_s1$name[i], ]
  }
}

# SNPs removed
d3_s1 = dim(geno_s1)[1]
print((d2_s1 - d3_s1) / 83)
```

```{r}
# S2 - Remove SNPs with z.HWE > 3.3 (p = 0.001) in controls
controls_s2 <- pheno_s2$obese == 0 & !is.na(pheno_s2$obese)
geno_controls_s2 <- geno_s2[controls_s2, ]
info_snsp_controls_s2 <- col.summary(geno_controls_s2)

use_s2 <- abs(info_snsp_controls_s2$z.HWE < 3.3)
mask.snps_s2 <- use_s2 & !is.na(use_s2)
geno_s2 <- geno_s2[, mask.snps_s2]
annotation_s2 <- annotation_s2[mask.snps_s2, ]

# Number of SNPs removed for HWE test
sum(info_snsp_controls_s2$z.HWE < 3.3, na.rm = TRUE)
```

#### Remove SNPs with low GC score
At one SNP level:
```{r}
# S1 - Check how many GC scores are above 0.6 for the first SNP
length(which(first_snp_s1$gcscore < 0.6))

# Check the first SNP median GC score
median(first_snp_s1$gcscore)
```

So, the SNP we are analyzing is fine.

At genome-wide level:
```{r}
# S1 - Remove SNPs with GC score < 0.6 for the first 20 SNPs
for (i in 1:20) {
  one_snp_s1 = geno_s1[geno_s1$snp == annotation_s1$name[i], ]
  if (isTRUE(one_snp_s1$gcscore < 0.6)) {
    geno_s1 <- geno_s1[geno_s1$snp != annotation_s1$name[i], ]
    annotation_s1 <-
      annotation_s1[annotation_s1$name != annotation_s1$name[i], ]
  }
}

# SNPs removed
d4_s1 = dim(geno_s1)[1]
print((d3_s1 - d4_s1) / 83)
```

### QC of individuals

* The identification of individuals with discordant reported and genomic sex: Males have expected heterozygosity of chromosome X of 0 and females of 0.30.
* The identification of individuals with a missing rate of 2 or 3% of the genotypes.
* The identification of individuals with low median GC scores (< 0.6 - 0.8).
* The identification of individuals with outlying heterozygosity: It is simply the proportion of heterozygotes in relation to all genotypes. If a sample’s heterozygosity is too high it can be an indication of DNA contamination. Removal of samples ±3 SD from the mean is reasonable.
* The identification of duplicated or related individuals: Random genotypes in HWE in real populations, on average, should show a correlation of < 0.3 – 0.6.
* The identification of individuals of divergent ancestry from the sample.

#### Remove individuals with sex discrepancies
At genome-wide level:
```{r}
# S2 - Remove individuals with sex discrepancies
# Import packages
library(SNPRelate)
library(SNPassoc)

# Identifying individuals with sex discrepancies
geno.X_s2 <- geno_s2[, annotation_s2$chromosome == "23" &
                       !is.na(annotation_s2$chromosome)]
info.X_s2 <- row.summary(geno.X_s2)
mycol <- ifelse(pheno_s2$gender == "Male", "gray40", "gray80")
sex.discrep_s2 <-
  (pheno_s2$gender == "Male" &
     info.X_s2$Heterozygosity > 0.2) |
  (pheno_s2$gender == "Female" & info.X_s2$Heterozygosity < 0.2)

# Remove individuals with sex discrepancies
use_s2 <- !sex.discrep_s2
mask.indiv_s2 <- use_s2 & !is.na(use_s2)
geno_s2 <- geno_s2[mask.indiv_s2, ]
pheno_s2 <- pheno_s2[mask.indiv_s2, ]

# Number of individuals removed for sex discrepancies
sum(sex.discrep_s2)
```

#### Remove individuals with outlying heterozygosity
At genome-wide level:
```{r}
# S1 - Remove individuals with outlying heterozygosity
# Build a matrix with the genotype counts for each sample
sumslides_s1 = matrix(NA, length(unique(geno_s1$animal)), 4)
rownames(sumslides_s1) = unique(geno_s1$animal)
colnames(sumslides_s1) = c("-/-", "A/A", "A/B", "B/B")

# Build a matrix of SNP data for each sample
numgeno_s1 = matrix(9, dim(annotation_s1)[1], length(unique(geno_s1$animal)))
for (i in 1:length(unique(geno_s1$animal))) {
  ## Extract data for each sample
  hold_s1 = geno_s1[geno_s1$animal == unique(geno_s1$animal)[i], ]
  ## Sort it by SNP
  hold_s1 = hold_s1[order(hold_s1$snp), ]
  ## Summarize the genotypic data and add the results to sumslides
  sumslides_s1[i, ] = summary(hold_s1$genotype)
  hold_s1$genotype <- addNA(hold_s1$genotype)
  temp_s1 = hold_s1$genotype
  ## Re-level genotypes into numeric format (9—missing, 0—AA, 1—AB, 2—BB)
  levels(temp_s1) = c(0, 1, 2, 9)
  numgeno_s1[, i] = as.numeric(as.character(temp_s1))
}

# Assign names to the rows and columns
rownames(numgeno_s1) = hold_s1$snp
colnames(numgeno_s1) = unique(geno_s1$animal)

# Divide the number of heterozygotes by all genotypes
samplehetero_s1 = sumslides_s1[, 3] / (sumslides_s1[, 2] + sumslides_s1[, 3] + sumslides_s1[, 4])

# Calculate the mean and standard deviation
mean_var_s1 = mean(samplehetero_s1)
sd_var_s1 = sd(samplehetero_s1)

# Calculate the values for 3SD to each side of the mean
up_s1 = mean_var_s1 + 3 * sd_var_s1
down_s1 = mean_var_s1 - 3 * sd_var_s1

# Number of outliers
hsout_up_s1 = length(which(samplehetero_s1 > up_s1))
hsout_down_s1 = length(which(samplehetero_s1 < down_s1))
hsout_s1 = hsout_up_s1 + hsout_down_s1

# Plot heterozygosity
plot(
  sort(samplehetero_s1),
  1:length(unique(geno_s1$animal)),
  col = "blue",
  cex.main = 1,
  cex.axis = 0.8,
  cex.lab = 1,
  ylab = "Sample",
  xlab = "Heterozygosity",
  yaxt = "n",
  main = paste(
    "Sample Heterozygosity\n",
    "Mean:",
    round(mean_var_s1, 3),
    " SD:",
    round(sd_var_s1, 3)
  ),
  sub = paste(
    "(Mean: Black line",
    ", 3SD: Red line",
    ", Number of outliers:",
    hsout_s1,
    ")"
  ),
  cex.sub = 0.8
)
abline(v = mean_var_s1)
abline(v = mean_var_s1 - 3 * sd_var_s1, col = "red")
abline(v = mean_var_s1 + 3 * sd_var_s1, col = "red")

# Remove samples with outlying heterozygosities
for (i in 1:length(samplehetero_s1)) {
  if (isTRUE(samplehetero_s1[i] > up_s1)) {
    geno_s1 = geno_s1[geno_s1$animal != names(samplehetero_s1[i])]
  }
  else if (isTRUE(samplehetero_s1[i] < down_s1)) {
    geno_s1 = geno_s1[geno_s1$animal != names(samplehetero_s1[i])]
  }
}
```

```{r}
# S2 - Remove individuals with outlying heterozygosity
# Identifying individuals
info.indv_s2 <- row.summary(geno_s2)
MAF_s2 <- col.summary(geno_s2)$MAF
callmatrix_s2 <- !is.na(geno_s2)
hetExp_s2 <- callmatrix_s2 %*% (2 * MAF_s2 * (1 - MAF_s2))
hetObs_s2 <-
  with(info.indv_s2, Heterozygosity * (ncol(geno_s2)) * Call.rate)
info.indv_s2$hetF <- 1 - (hetObs_s2 / hetExp_s2)

# Remove individuals with outlying heterozygosity > 0.1
use_s2 <- abs(info.indv_s2$hetF) < 0.1
mask.indiv_s2 <- use_s2 & !is.na(use_s2)
geno_s2 <- geno_s2[mask.indiv_s2, ]
pheno_s2 <- pheno_s2[mask.indiv_s2, ]

# Number of individuals removed for heterozygosity problems
sum(abs(info.indv_s2$hetF) > 0.1)
```

#### Remove duplicated or related individuals
At genome-wide level:
```{r}
# S2 - Remove duplicated or related individuals
# Identifying individuals
info.indv_s2 <- row.summary(geno_s2)
snpgdsBED2GDS(
  "Output/obesity.bed",
  "Data/obesity.fam",
  "Data/obesity.bim",
  out = "Output/obGDS",
  verbose = FALSE
)
genofile_s2 <- snpgdsOpen("Output/obGDS")
snps_s2 <- colnames(geno_s2)
snp.prune_s2 <-
  snpgdsLDpruning(
    genofile_s2,
    ld.threshold = 0.2,
    snp.id = snps_s2,
    verbose = FALSE
  )
snps.ibd_s2 <- unlist(snp.prune_s2, use.names = FALSE)
ibd_s2 <-
  snpgdsIBDMoM(
    genofile_s2,
    kinship = TRUE,
    snp.id = snps.ibd_s2,
    num.thread = 1,
    verbose = FALSE
  )
ibd.kin_s2 <- snpgdsIBDSelection(ibd_s2)
ibd.kin.thres_s2 <- subset(ibd.kin_s2, kinship > 0.1)
ids.rel_s2 <-  related(ibd.kin.thres_s2)

# Remove individuals with relatedness higher than > 0.1
use_s2 <- !rownames(info.indv_s2) %in% ids.rel_s2
mask.indiv_s2 <- use_s2 & !is.na(use_s2)
geno_s2 <- geno_s2[mask.indiv_s2, ]
pheno_s2 <- pheno_s2[mask.indiv_s2, ]

# Number of individuals removed to be related with others
length(ids.rel_s2)
```

### QC of Population ancestry:
Ancestral differences can be inferred with principal component analysis (PCA) on the genomic data. Individuals with outlying ancestry can be removed from the study while smaller differences in ancestry can be adjusted in the association models, including the first principal components as covariates.

At genome-wide level:
```{r}
# S2 - Check population ancestry
# Import packages
library(SNPRelate)

# Compute PCA on genomic data
pca <-
  snpgdsPCA(
    genofile_s2,
    sample.id = rownames(geno_s2),
    snp.id = snps.ibd_s2,
    num.thread = 1,
    verbose = FALSE
  )

# Obtain a PCA plot for the first two components
with(
  pca,
  plot(
    eigenvect[, 1],
    eigenvect[, 2],
    xlab = "1st Principal Component",
    ylab = "2nd Principal Component",
    main = "Ancestry Plot",
    pch = 21,
    bg = "gray90",
    cex = 0.8
  )
)

# Add the first five principal components to the phenotypic information
pheno_s2 <- data.frame(pheno_s2, pca$eigenvect[, 1:5])

# Close the GDS file
closefn.gds(genofile_s2)

# Remove extra file
system("rm Output/obGDS")
```

### Clean-up
At genome-wide level:
```{r}
# S1 - Clean-up
# Change the names of the factors from 0, 1 and 2 to AA, AB and BB
numgeno_s1 <- data.frame(numgeno_s1)
for (i in 1:length(numgeno_s1[1, ])) {
  numgeno_s1[, i] <- factor(numgeno_s1[, i])
  levels(numgeno_s1[, i]) = c("AA", "AB", "BB", NA)
}

# Remove SNP and/or samples with complete missing data
indexsnp_s1 = apply(numgeno_s1, 1, function(x) {
  length(which(is.na(x) == T))
})
indexsnp_s1 = which(indexsnp_s1 == length(numgeno_s1[1, ]))
indexsample_s1 = apply(numgeno_s1, 2,
                    function(x) {
                      length(which(is.na(x) == T))
                    })
indexsample_s1  = which(indexsample_s1  ==  length(numgeno_s1[, 1]))
numgeno_s1 = numgeno_s1[-indexsnp_s1, ]

# Create a trait for the sake of analysis and training
weight_s1 = rnorm(length(unique(colnames(numgeno_s1))), mean = 50, sd = 10)

# Check data
numgeno_s1[1:4, 1:4]
```

### Final notes on QC

* (1) What we discussed here was across all SNP and/or samples. With case–control studies it is worthwhile running these QC metrics independently on cases and controls and then checking the results for consistency.
* (2) We have not plotted any results based on mapping information, it is a good idea to plot, e.g., HWE statistics per chromosome to see if there are any evident patterns such as a block on the chromosome that is consistently out of HWE.

<br>

## GWAS
### Scenario 1

```{r}
# S1 - Analysis
# First we create a function to fit trait to the SNP
singlesnp = function(trait, snp) {
  if (length(levels(snp)) > 1) {
    lm(trait ~ snp)
    }
  else {
    NA
    }
}

# Apply the function to each SNP and store the model returned by lm in results
results_s1 = apply(numgeno_s1, 1, function(x) {
  singlesnp(weight_s1, factor(t(x)))
})

# Then we use lapply to extract the p-values for the anova test
pvalfunc = function(model) {
  if (class(model) == "lm")
    anova(model)[[5]][1]
  else
    NA
}

pvals_s1 = lapply(results_s1, function(x) {
  pvalfunc(x)
})

# Give names to rows and columns
names(results_s1) = row.names(numgeno_s1)
pvals_s1 = data.frame(snp = row.names(numgeno_s1), pvalue = unlist(pvals_s1))
rownames(pvals_s1) <- 1:length(rownames(pvals_s1))

# Check some p-values
pvals_s1[1:5, ]
```

The next step is to have a look at the effect sizes per genotype of the significant SNP:
```{r}
# S1 - Get the indexes of the top n p-values
n = 5
index_s1 = sort(pvals_s1$pvalue, index.return = T, na.last = NA)[[2]][1:n]

# Extract the coefficients from each of the top SNPs and build a table
estimates_s1 = NULL
for (i in 1:n) {
  estimates_s1 = rbind(estimates_s1,
                    coefficients(summary(results_s1[[index_s1[i]]])))
}

estimates_s1 = cbind(rep(c("AA mean", "AB dev", "BB dev"), n),
                  estimates_s1,
                  rep(names(results_s1)[index_s1], each = 3))
estimates_s1 = data.frame(estimates_s1)
names(estimates_s1) = c("Genotype", "Effect",
                     "SE", "t-value",
                     "p-value", "SNP")

for (i in 2:5) {
  estimates_s1[, i] = signif(as.numeric(as.character(estimates_s1[, i])), 2)
}

rownames(estimates_s1) <- 1:length(rownames(estimates_s1))

# Check data
print(estimates_s1)
```

Let's plot the negative log odds of the p-values for each chromosome for the association test by physical location.
```{r}
# S1 - Plot
# Merge the p-values with mapping information from our database
merged_s1 = merge(pvals_s1, annotation_s1, by.x = 1, by.y = 1)
merged_s1[1:4, ]

# Plot the log odds for chromosome n
n = 1
plot(
  merged_s1$position[which(merged_s1$chromosome == n)],
  -log(merged_s1$pvalue[which(merged_s1$chromosome == n)]),
  xlab = "Map position",
  ylab = "-log(odds)",
  cex.axis = 0.7,
  col = "blue",
  pch = 20,
  main = paste("Chromosome", n)
)

# Add a line for the significance threshold of 0.05
abline(h = -log(0.05), col = "orange", lty = 2, lwd = 4)

# Add a line for the significance threshold of 0.01
abline(h = -log(0.01), col = "red", lty = 2, lwd = 4)
```

#### Multiple testing
Bonferroni is easy, just divide the desired significance level for the test by the number of tests. A SNP with a p-value under that threshold is considered significant.

```{r}
# P-value for significance of "sig", after Bonferroni correction
sig = 0.01
sig_corrected_s1 = sig / length(pvals_s1[, 1])
print(sig_corrected_s1)
```

*Note: * The problem with Bonferroni is that it is overly conservative.

Let’s see how many of our significant SNPs survived multiple testing correction:
```{r}
# Our top p-values
sort(pvals_s1$pvalue)[1:5]

# Without correction
length(which(pvals_s1$pvalue < sig))

# With correction
length(which(pvals_s1$pvalue < sig_corrected_s1))
```

### Scenario 2
It involves regressing each SNP separately on our trait of interest:

```{r}
# Import packages
library(snpStats)

# Analysis for the obesity using the additive and the codominant models
res <- single.snp.tests(obese, data = pheno_s2, snp.data = geno_s2)
res[1:5, ]
```

```{r}
# Analysis for age using the additive model
res.age <-
  snp.rhs.tests(age ~ 1,
                data = pheno_s2,
                snp.data = geno_s2,
                family = "Gaussian")
head(res.age)
```

#### Adjusting for population stratification
The inflation of the associations due to undetected latent variables is assessed by quantile-quantile (Q-Q) plots where observed chi2 values are plotted against the expected ones. A Q-Q plot with top SNPs outside the confidence bands indicates that those SNPs are truly associated with the disease and, hence, do not follow the null hypothesis.

```{r}
# QQ-plot
chi2_s2 <- chi.squared(res, df=1)
qq.chisq(chi2_s2)
```

```{r}
# Adjust for population stratification using the PCs on genomic data to infer
# ancestral differences in the sample
res.adj <-
  snp.rhs.tests(obese ~ X1 + X2 + X3 + X4 + X5, data = pheno_s2, snp.data =
                  geno_s2)
head(res.adj)
```

#### Manhattan plot

```{r}
# Manhattan plot function
manhattanPlot <- function(x,
                          colors = c("grey", "skyblue"),
                          significantLine = 8,
                          snpsOfInterest = NULL,
                          ...) {
  don <- x %>%
    
    # Compute chromosome size
    group_by(CHR) %>%
    dplyr::summarise(chr_len = max(BP)) %>%
    
    # Calculate cumulative position of each chromosome
    mutate(tot = cumsum(as.numeric(chr_len)) - chr_len) %>%
    dplyr::select(-chr_len) %>%
    
    # Add this info to the initial dataset
    left_join(x, ., by = c("CHR" = "CHR")) %>%
    
    # Add a cumulative position of each SNP
    arrange(CHR, BP) %>%
    mutate(BPcum = BP + tot) %>%
    
    # Add highlight and annotation information
    mutate(is_highlight = ifelse(SNP %in% snpsOfInterest, "yes", "no")) %>%
    mutate(is_annotate = ifelse(-log10(P) > 8, "yes", "no"))
  
  # Prepare X axis
  axisdf <-
    don %>% group_by(CHR) %>% dplyr::summarize(center = (max(BPcum) + min(BPcum)) / 2)
  
  # Make the plot
  ggplot(don, aes(x = BPcum, y = -log10(P))) +
    
    # Show all points
    geom_point(aes(color = as.factor(CHR)), alpha = 0.8, size = 1.3) +
    scale_color_manual(values = rep(colors, 22)) +
    
    # custom X axis:
    scale_x_continuous(label = axisdf$CHR, breaks = axisdf$center) +
    scale_y_continuous(expand = c(0, 0)) +     # remove space between plot area and x axis
    
    # Add highlighted points
    geom_point(
      data = subset(don, is_highlight == "yes"),
      color = "orange",
      size = 2
    ) +
    
    # Add label using ggrepel to avoid overlapping
    geom_label_repel(data = subset(don, is_annotate == "yes"),
                     aes(label = SNP),
                     size = 2) +
    
    # Change X-legend
    xlab("Chromosome") +
    
    # Add genome-wide line
    geom_hline(yintercept = significantLine, linetype = "dashed") +
    
    # Custom the theme:
    theme_bw() +
    theme(
      legend.position = "none",
      panel.border = element_blank(),
      panel.grid.major.x = element_blank(),
      panel.grid.minor.x = element_blank()
    )
}
```

```{r}
# Extract the resulting - log10(P-values) of association for each SNP
pval.log10 <- -log10(p.value(res.adj))

# Create a Manhattan plot
library(tidyverse)
library(ggplot2)
library(ggrepel)
pvals_s2 <-
  data.frame(
    SNP = annotation_s2$snp.name,
    CHR = annotation_s2$chromosome,
    BP = annotation_s2$position,
    P = p.value(res.adj)
  )
pvals_s2 <- subset(pvals_s2,!is.na(CHR) & !is.na(P))

manhattanPlot(pvals_s2, color = c("gray90", "gray40"))
```

### Scenario 3

```{r}
# t-test
library(genefilter)
ind_tt = rowttests(ind_y, ind_g)

# How many are sig at p < 0.01
sum(ind_tt$p.value < 0.01)
```

#### Multiple comparisons

```{r}
# Address the issue using the q-value
library(qvalue)
qvals = qvalue(ind_tt$p.value)$qvalue
sum(qvals < 0.01)
```

## Extra Scenario (limma hierarchical model)
### Simple t-tests

```{r}
# Import data
library(SpikeInSubset)
data(rma95)

# Check pheno data for the first 5 mRNAs
pData(rma95)[, 1:5]

# Check seq data for five mRNAs
exprs(rma95)[655:659, ]

# Simple t-tests
library(genefilter)
fac <- factor(rep(1:2, each = 3))
rtt <- rowttests(exprs(rma95), fac)

# Define colors depending on whether the p-value is small, 
# the absolute difference in means is large,
# and whether the feature is a spike-in value
mask <- with(rtt, abs(dm) < 0.2 & p.value < 0.01)
mask[140:160]
spike <- rownames(rma95) %in% colnames(pData(rma95))
spike[20:40]

# The spike-in genes are in blue, while the red points indicate genes which have
# small p-values but also small differences in means.
cols <- ifelse(mask, "red", ifelse(spike, "dodgerblue", "black"))
cols[20:30]

# Plot p-values against DM
with(
  rtt,
  plot(
    -dm,
    -log10(p.value),
    cex = .8,
    pch = 16,
    xlim = c(-1, 1),
    ylim = c(0, 5),
    xlab = "difference in means",
    col = cols
  )
)
abline(h = 2, v = c(-.2, .2), lty = 2)

# Plot p-values against SD
rtt$s <-
  apply(exprs(rma95), 1, function(row)
    sqrt(.5 * (var(row[1:3]) + var(row[4:6]))))
with(
  rtt,
  plot(
    s,
    -log10(p.value),
    cex = .8,
    pch = 16,
    log = "x",
    xlab = "estimate of standard deviation",
    col = cols
  )
)
```

### limma steps

```{r}
library(limma)

# Step 1: least squares estimates
fit <- lmFit(rma95, design = model.matrix( ~ fac))

# Step 2: moderate the t statistics
fit <- eBayes(fit)

# Step 3: report
topTable(fit, coef = 2)
```

**Note:** *topTable* will return the top genes ranked by whichever value you define.

**Note:** The column *adj.P.Val* includes the adjusted p-values (using the method of Benjamini-Hochberg) for each gene.

```{r}
# Plot p-values against DM
limmares <-
  data.frame(dm = coef(fit)[, "fac2"], p.value = fit$p.value[, "fac2"])
with(
  limmares,
  plot(
    dm,
    -log10(p.value),
    cex = .8,
    pch = 16,
    col = cols,
    xlab = "difference in means",
    xlim = c(-1, 1),
    ylim = c(0, 5)
  )
)
abline(h = 2, v = c(-.2, .2), lty = 2)
```

Note that the red points are now all under the line where -log10(p.value) is equal to 2. Also, the blue points which represent real differences have p-values which are even higher than before.

# GWAS based on haplotypes
Haplotypes are blocks of correlated SNPs that tend to be inherited together (e.g., HLA alleles in the major histocompatibility complex).

## Scenario
To assess if any of the haplotypes in our data significantly associate with asthma.

## Load data
```{r}
asthma <- read.table("Data/asthma.txt", header = TRUE)

# Check data's first 20 headers (the rest are SNPs as well)
colnames(asthma)[1:20]

# Check some samples
t(asthma[1:3, 1:7])

# Check some SNPs
asthma[1:5, 7:9]
```

## Preprocessing
To start the analysis, we must indicate which columns of the dataset asthma contain the SNP data, using the *setupSNP* function. In our example, SNPs start from column 7 onwards, which we specify in argument *colSNPs*:
```{r}
library(SNPassoc)
asthma.s <- setupSNP(data = asthma,
                     colSNPs = 7:ncol(asthma),
                     sep = "")

# Check SNP data now
asthma.s[1:5, 7:9]

# Check the genotype and allele frequencies for a SNP
summary(asthma.s$rs1422993)

# Plot missing genotypes across all SNPs
plotMissing(asthma.s, print.labels.SNPs = TRUE)
```

Let's check for H-W equilibrium and keep those SNPs that do not reject the null hypothesis (p > 0.05):
```{r}
hwe <- tableHWE(asthma.s, casecontrol)

# Check some results
hwe[1:4,]

# Keeping SNPs that pass the H-W test
snps.ok <- rownames(hwe)[hwe[, 2] >= 0.05]
pos <- which(colnames(asthma) %in% snps.ok, useNames = FALSE)
asthma.s <- setupSNP(data = asthma,
                     colSNPs = pos,
                     sep = "")
```

Now, let's check the Linkage disequilibrium heatmap of SNPs located in the region 34.5-35.0Mb of chromosome 7, for which we create a snp object:
```{r}
library(LDheatmap)
library(genetics)
library(biomaRt)
mart <- useMart("ENSEMBL_MART_SNP",
                dataset = "hsapiens_snp",
                host = "https://useast.ensembl.org")
snps <- labels(asthma.s)
snpInfo <-
  getBM(
    c("refsnp_id", "chr_name", "chrom_start", "allele"),
    filters = c("snp_filter"),
    values = snps,
    mart = mart
  )
head(snpInfo)

mask <-
  with(snpInfo,
       chr_name == "7" & chrom_start > 34.5e6 & chrom_start < 35.0e6)
snps.sel <- snpInfo[mask, "refsnp_id"]
sel <- which(names(asthma) %in% snps.sel)
asthma.hap <- setupSNP(asthma, sel, sep = "")
snp.pos <- snpInfo[mask, "chrom_start"]
snp.geno <- data.frame(lapply(asthma.hap[, snps.sel], genotype))
LDplot <-
  LDheatmap(
    snp.geno,
    LDmeasure = "r",
    title = "Pairwise LD in r^2",
    add.map = TRUE,
    color = grey.colors(20),
    name = "myLDgrob",
    add.key = TRUE,
    flip = TRUE,
    SNP.name = snps.sel
  )
```

## Haplotype analysis
Now, let's do the analysis. First, let's do haplotype estimation:
```{r}
library(haplo.stats)
trait <- asthma.hap$casecontrol
snpsH <- c("rs714588", "rs1023555",  "rs898070")
genoH <- make.geno(asthma.hap, snpsH)
em <- haplo.em(genoH, locus.label = snpsH, miss.val = c(0, NA))
em
```

Now, haplotype association:
```{r}
mod <-
  haplo.glm(
    trait ~ genoH,
    family = "binomial",
    locus.label = snpsH,
    allele.lev = attributes(genoH)$unique.alleles,
    control = haplo.glm.control(haplo.freq.min = 0.05)
  )

intervals(mod)
```

When no previous knowledge is available about the region or SNPs for which haplotypes should be inferred, we can apply a sliding window for haplotype inference.
```{r}
snpsH2 <- colnames(snp.geno)[6:15]
genoH2 <- make.geno(asthma.hap, snpsH2)
haplo.score <- list()
for (i in 4:7) {
  trait <- asthma.hap$casecontrol
  haplo.score[[i - 3]] <-
    haplo.score.slide(
      trait,
      genoH2,
      trait.type = "binomial",
      n.slide = i,
      simulate = TRUE,
      sim.control = score.sim.control(min.sim = 100, max.sim = 200)
    )
}

par(mfrow = c(2, 2))
for (i in 4:7) {
  plot(haplo.score[[i - 3]])
  title(paste("Sliding Window=", i, sep = ""))
}

snpsH3 <- snpsH2[4:7]
genoH3 <- make.geno(asthma.hap, snpsH3)
mod <-
  haplo.glm(
    trait ~ genoH3,
    family = "binomial",
    locus.label = snpsH3,
    allele.lev = attributes(genoH3)$unique.alleles,
    control = haplo.glm.control(haplo.freq.min = 0.05)
  )
intervals(mod)
```

A likelihood ratio test for haplotype status can be extracted from the results of the haplo.glme function:
```{r}
lrt <- mod$lrt
pchisq(lrt$lrt, lrt$df, lower = FALSE)
```

We can also test the association between asthma and the haplotype adjusted for smoking status:
```{r}
smoke <- asthma.hap$smoke
mod.adj.ref <- glm(trait ~ smoke, family = "binomial")
mod.adj <-
  haplo.glm(
    trait ~ genoH3 + smoke ,
    family = "binomial",
    locus.label = snpsH3,
    allele.lev = attributes(genoH3)$unique.alleles,
    control = haplo.glm.control(haplo.freq.min = 0.05)
  )
lrt.adj <- mod.adj.ref$deviance - mod.adj$deviance
pchisq(lrt.adj, mod.adj$lrt$df, lower = FALSE)
```

```{bash, echo=FALSE}
rm Output/obesity.bed
```
